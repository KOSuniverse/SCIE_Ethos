{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNASvGzFZLXmybY6F7/vDNn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwHfmMRhf-Fz","executionInfo":{"status":"ok","timestamp":1754407070451,"user_tz":300,"elapsed":17029,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}},"outputId":"4af8f5cc-dbe7-4a53-f4df-66efa996ad17"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# 1️⃣ Mount Google Drive (Colab only)\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install python-docx pdfplumber python-pptx unstructured pytesseract\n","!apt install tesseract-ocr\n","# 1. Install pdf2image\n","!pip install pdf2image\n","\n","# 2. Install poppler (required for pdf2image to work)\n","!apt-get install -y poppler-utils\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3V71cnPCgPCk","executionInfo":{"status":"ok","timestamp":1754407103583,"user_tz":300,"elapsed":33131,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}},"outputId":"5614d7bf-e95a-4926-d36b-806085ec91bf"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-docx\n","  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n","Collecting pdfplumber\n","  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m446.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-pptx\n","  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n","Collecting unstructured\n","  Downloading unstructured-0.18.11-py3-none-any.whl.metadata (24 kB)\n","Collecting pytesseract\n","  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n","Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n","Collecting pdfminer.six==20250506 (from pdfplumber)\n","  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n","Collecting pypdfium2>=4.18.0 (from pdfplumber)\n","  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n","Collecting XlsxWriter>=0.5.7 (from python-pptx)\n","  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n","Collecting filetype (from unstructured)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting python-magic (from unstructured)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.32.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.4)\n","Collecting emoji (from unstructured)\n","  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n","Collecting dataclasses-json (from unstructured)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting python-iso639 (from unstructured)\n","  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n","Collecting langdetect (from unstructured)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.0.2)\n","Collecting rapidfuzz (from unstructured)\n","  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting backoff (from unstructured)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Collecting unstructured-client (from unstructured)\n","  Downloading unstructured_client-0.42.0-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n","Collecting python-oxmsg (from unstructured)\n","  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.7)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (1.17.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (2024.11.6)\n","Collecting olefile (from python-oxmsg->unstructured)\n","  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2025.7.14)\n","Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (24.1.0)\n","Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.0.9)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (0.28.1)\n","Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (2.11.7)\n","Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n","  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.0.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured) (0.16.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n","Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured-0.18.11-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n","Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n","Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured_client-0.42.0-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.1/207.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=3738523a8cf012c1a2b54cbac58b94a507028d01d1151e408cdbd0b19098ba48\n","  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n","Successfully built langdetect\n","Installing collected packages: filetype, XlsxWriter, rapidfuzz, python-magic, python-iso639, python-docx, pytesseract, pypdfium2, pypdf, olefile, mypy-extensions, marshmallow, langdetect, emoji, backoff, typing-inspect, python-pptx, python-oxmsg, unstructured-client, pdfminer.six, dataclasses-json, unstructured, pdfplumber\n","Successfully installed XlsxWriter-3.2.5 backoff-2.2.1 dataclasses-json-0.6.7 emoji-2.14.1 filetype-1.2.0 langdetect-1.0.9 marshmallow-3.26.1 mypy-extensions-1.1.0 olefile-0.47 pdfminer.six-20250506 pdfplumber-0.11.7 pypdf-5.9.0 pypdfium2-4.30.0 pytesseract-0.3.13 python-docx-1.2.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 python-pptx-1.0.2 rapidfuzz-3.13.0 typing-inspect-0.9.0 unstructured-0.18.11 unstructured-client-0.42.0\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","tesseract-ocr is already the newest version (4.1.1-2.1build1).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","Collecting pdf2image\n","  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.3.0)\n","Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n","Installing collected packages: pdf2image\n","Successfully installed pdf2image-1.17.0\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  poppler-utils\n","0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 186 kB of archives.\n","After this operation, 697 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.9 [186 kB]\n","Fetched 186 kB in 2s (123 kB/s)\n","Selecting previously unselected package poppler-utils.\n","(Reading database ... 126284 files and directories currently installed.)\n","Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.9_amd64.deb ...\n","Unpacking poppler-utils (22.02.0-2ubuntu0.9) ...\n","Setting up poppler-utils (22.02.0-2ubuntu0.9) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import uuid\n","import textwrap\n","from pathlib import Path\n","import pytesseract\n","from PIL import Image\n","from docx import Document\n","from pptx import Presentation\n","import pdfplumber\n","from pdf2image import convert_from_path"],"metadata":{"id":"0Lzn-nORpvS0","executionInfo":{"status":"ok","timestamp":1754407104130,"user_tz":300,"elapsed":534,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ✅ Folder setup\n","BASE_DIR = Path(\"/content/drive/MyDrive/Ethos LLM/Project_Root/06_LLM_Knowledge_Base\")\n","OUTPUT_DIR = BASE_DIR / \"_metadata\"\n","OUTPUT_DIR.mkdir(exist_ok=True)\n","\n","SUPPORTED_EXTS = [\".pdf\", \".docx\", \".pptx\"]"],"metadata":{"id":"YAaPdZlSpyn6","executionInfo":{"status":"ok","timestamp":1754407105824,"user_tz":300,"elapsed":1690,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# ✅ 1. Get all documents\n","def get_all_documents(base_dir=BASE_DIR):\n","    files = []\n","    for subfolder in base_dir.iterdir():\n","        if subfolder.is_dir() and subfolder.name != \"_metadata\":\n","            for file in subfolder.rglob(\"*\"):\n","                if file.suffix.lower() in SUPPORTED_EXTS:\n","                    files.append({\n","                        \"path\": file,\n","                        \"category\": subfolder.name,\n","                        \"filename\": file.name\n","                    })\n","    return files"],"metadata":{"id":"WzFys-wvp2Ag","executionInfo":{"status":"ok","timestamp":1754407105850,"user_tz":300,"elapsed":13,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ✅ 2. Text extractors (with smart OCR fallback and logging)\n","\n","from pdf2image import convert_from_path\n","import pdfplumber\n","import pytesseract\n","from docx import Document\n","from pptx import Presentation\n","\n","def extract_text_from_pdf(file_path, min_text_length=30):\n","    full_text = \"\"\n","\n","    try:\n","        with pdfplumber.open(file_path) as pdf:\n","            total_pages = len(pdf.pages)\n","\n","            for i, page in enumerate(pdf.pages):\n","                print(f\"🧾 [{file_path.name}] Extracting page {i+1}/{total_pages}...\")\n","\n","                combined_text = \"\"\n","\n","                # 1. Try standard text extraction\n","                text_only = page.extract_text() or \"\"\n","\n","                if len(text_only.strip()) >= min_text_length:\n","                    # ✅ Enough text found, no OCR needed\n","                    combined_text = text_only.strip()\n","                else:\n","                    # ⚠️ Run OCR if text is missing or too short\n","                    print(f\"🔍 Running OCR on page {i+1}/{total_pages} of {file_path.name}...\")\n","                    try:\n","                        images = convert_from_path(\n","                            str(file_path), dpi=300,\n","                            first_page=i+1, last_page=i+1\n","                        )\n","                        ocr_text = pytesseract.image_to_string(images[0])\n","                        combined_text = f\"[OCR only:]\\n{ocr_text.strip()}\"\n","                    except Exception as e:\n","                        print(f\"⚠️ OCR failed on page {i+1} of {file_path.name}: {e}\")\n","                        combined_text = \"\"\n","\n","                full_text += combined_text + \"\\n\\n\"\n","\n","    except Exception as e:\n","        print(f\"❌ Failed to open PDF {file_path.name}: {e}\")\n","        return \"\"\n","\n","    return full_text\n","\n","\n","def extract_text_from_docx(file_path):\n","    try:\n","        doc = Document(file_path)\n","        return \"\\n\".join([para.text for para in doc.paragraphs])\n","    except Exception as e:\n","        print(f\"Error reading DOCX {file_path}: {e}\")\n","        return \"\"\n","\n","\n","def extract_text_from_pptx(file_path):\n","    try:\n","        prs = Presentation(file_path)\n","        text_runs = []\n","        for slide in prs.slides:\n","            for shape in slide.shapes:\n","                if hasattr(shape, \"text\"):\n","                    text_runs.append(shape.text)\n","        return \"\\n\".join(text_runs)\n","    except Exception as e:\n","        print(f\"Error reading PPTX {file_path}: {e}\")\n","        return \"\"\n","\n","\n","def extract_text(file_record):\n","    path = file_record[\"path\"]\n","    ext = path.suffix.lower()\n","    if ext == \".pdf\":\n","        return extract_text_from_pdf(path)\n","    elif ext == \".docx\":\n","        return extract_text_from_docx(path)\n","    elif ext == \".pptx\":\n","        return extract_text_from_pptx(path)\n","    else:\n","        return \"\"\n"],"metadata":{"id":"gbrWwC2AQnsw","executionInfo":{"status":"ok","timestamp":1754407105870,"user_tz":300,"elapsed":18,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ✅ 3. Chunking\n","def chunk_text(text, chunk_size=800, overlap=100):\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = start + chunk_size\n","        chunks.append(text[start:end])\n","        start = end - overlap\n","    return chunks"],"metadata":{"id":"J2VIDZCFqC6D","executionInfo":{"status":"ok","timestamp":1754407105875,"user_tz":300,"elapsed":3,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def load_processed_files(jsonl_path):\n","    processed_files = set()\n","    if os.path.exists(jsonl_path):\n","        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n","            for line in f:\n","                try:\n","                    record = json.loads(line)\n","                    processed_files.add(record[\"filename\"])\n","                except:\n","                    continue\n","    return processed_files\n"],"metadata":{"id":"pixyDpoYK9sF","executionInfo":{"status":"ok","timestamp":1754407105891,"user_tz":300,"elapsed":14,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def process_documents_to_chunks_streamed(output_path=OUTPUT_DIR / \"document_chunks.jsonl\"):\n","    all_files = get_all_documents()\n","    processed_files = load_processed_files(output_path)\n","\n","    print(f\"🗂 Found {len(all_files)} files. Skipping {len(processed_files)} already processed...\")\n","\n","    # Open file once for streaming write\n","    with open(output_path, \"a\", encoding=\"utf-8\") as f:  # use 'append' mode!\n","        for file_record in tqdm(all_files, desc=\"Processing docs\"):\n","            if file_record[\"filename\"] in processed_files:\n","                continue\n","\n","            print(f\"📄 Now processing: {file_record['category']} → {file_record['filename']}\")\n","\n","            try:\n","                raw_text = extract_text(file_record)\n","                chunks = chunk_text(raw_text)\n","\n","                for i, chunk in enumerate(chunks):\n","                    chunk_data = {\n","                        \"id\": str(uuid.uuid4()),\n","                        \"text\": chunk.strip(),\n","                        \"chunk_index\": i,\n","                        \"filename\": file_record[\"filename\"],\n","                        \"category\": file_record[\"category\"],\n","                        \"source_path\": str(file_record[\"path\"]),\n","                    }\n","                    json.dump(chunk_data, f)\n","                    f.write(\"\\n\")\n","\n","            except Exception as e:\n","                print(f\"❌ Failed to process {file_record['filename']}: {e}\")\n"],"metadata":{"id":"uHMwV7sWJ2y1","executionInfo":{"status":"ok","timestamp":1754407105978,"user_tz":300,"elapsed":85,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["!pip install faiss-cpu\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLdjnWonHHxH","executionInfo":{"status":"ok","timestamp":1754407112577,"user_tz":300,"elapsed":6596,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}},"outputId":"5008c628-9fb9-4560-b633-1ee325c5547b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n","Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.11.0.post1\n"]}]},{"cell_type":"code","source":["import faiss\n","import json\n","import numpy as np\n","from openai import OpenAI\n","\n","client = OpenAI(api_key=\"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\")  # Automatically uses your API key from environment\n","\n","# Load chunks from .jsonl\n","def load_chunks(jsonl_path):\n","    chunks = []\n","    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            chunks.append(json.loads(line))\n","    return chunks\n","\n","# Batch OpenAI embeddings using the new API client\n","def embed_texts(texts, model=\"text-embedding-3-small\"):\n","    embeddings = []\n","    for i in range(0, len(texts), 20):\n","        batch = texts[i:i+20]\n","        try:\n","            response = client.embeddings.create(input=batch, model=model)\n","            vectors = [d.embedding for d in response.data]\n","            embeddings.extend(vectors)\n","        except Exception as e:\n","            print(f\"❌ Embedding error at batch {i}: {e}\")\n","    return np.array(embeddings).astype(\"float32\")\n","\n","# Create FAISS index and save metadata\n","def build_faiss_index(\n","    chunks,\n","    index_path=OUTPUT_DIR / \"document_index.faiss\",\n","    metadata_path=OUTPUT_DIR / \"document_metadata.json\"\n","):\n","    texts = [chunk[\"text\"] for chunk in chunks]\n","    vectors = embed_texts(texts)\n","\n","    if len(vectors) == 0:\n","        print(\"❌ No embeddings were generated. Check for OpenAI errors.\")\n","        return\n","\n","    dim = len(vectors[0])\n","    index = faiss.IndexFlatL2(dim)\n","    index.add(vectors)\n","\n","    faiss.write_index(index, str(index_path))\n","\n","    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(chunks, f, indent=2)\n","\n","    print(f\"✅ FAISS index and metadata saved to:\\n{index_path}\\n{metadata_path}\")\n"],"metadata":{"id":"0xbzaCrPgPyv","executionInfo":{"status":"ok","timestamp":1754407113839,"user_tz":300,"elapsed":1249,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","# Step 1: Generate chunks\n","process_documents_to_chunks_streamed()\n","\n","\n","# Step 2: Load and embed\n","chunks = load_chunks(OUTPUT_DIR / \"document_chunks.jsonl\")\n","build_faiss_index(chunks)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iR43-zqPqM2Y","executionInfo":{"status":"ok","timestamp":1754407636239,"user_tz":300,"elapsed":522387,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}},"outputId":"7d292ea1-e21e-438d-9a41-86bca72a5e4f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["🗂 Found 17 files. Skipping 17 already processed...\n"]},{"output_type":"stream","name":"stderr","text":["Processing docs: 100%|██████████| 17/17 [00:00<00:00, 96616.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ FAISS index and metadata saved to:\n","/content/drive/MyDrive/Ethos LLM/Project_Root/06_LLM_Knowledge_Base/_metadata/document_index.faiss\n","/content/drive/MyDrive/Ethos LLM/Project_Root/06_LLM_Knowledge_Base/_metadata/document_metadata.json\n"]}]},{"cell_type":"code","source":["import faiss\n","import openai\n","import json\n","import numpy as np\n","from pathlib import Path\n","\n","# Paths\n","BASE_PATH = Path(\"/content/drive/MyDrive/Ethos LLM/Project_Root/06_LLM_Knowledge_Base/_metadata\")\n","CHUNKS_PATH = BASE_PATH / \"document_chunks.jsonl\"\n","INDEX_PATH = BASE_PATH / \"document_index.faiss\"\n","METADATA_PATH = BASE_PATH / \"document_metadata.json\"\n","\n","# Load chunks\n","def load_chunks(path):\n","    chunks = []\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            chunks.append(json.loads(line))\n","    return chunks\n","\n","# Embed with OpenAI\n","def embed_texts(texts, model=\"text-embedding-3-small\"):\n","    vectors = []\n","    for i in range(0, len(texts), 20):\n","        batch = texts[i:i+20]\n","        try:\n","            response = openai.Embedding.create(input=batch, model=model)\n","            vectors.extend([d[\"embedding\"] for d in response[\"data\"]])\n","        except Exception as e:\n","            print(f\"❌ Embedding failed at batch {i}: {e}\")\n","    return np.array(vectors).astype(\"float32\")\n","\n","# Build FAISS index\n","def build_faiss_index(chunks, index_path=INDEX_PATH, metadata_path=METADATA_PATH):\n","    texts = [chunk[\"text\"] for chunk in chunks]\n","    vectors = embed_texts(texts)\n","    index = faiss.IndexFlatL2(len(vectors[0]))\n","    index.add(vectors)\n","\n","    faiss.write_index(index, str(index_path))\n","    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(chunks, f, indent=2)\n","\n","    print(f\"✅ Saved FAISS index to {index_path}\")\n","    print(f\"✅ Saved metadata to {metadata_path}\")\n"],"metadata":{"id":"3EBB5DYXqM0O","executionInfo":{"status":"ok","timestamp":1754407636241,"user_tz":300,"elapsed":13,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Reload FAISS index and metadata from disk\n","index = faiss.read_index(str(INDEX_PATH))\n","with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as f:\n","    metadata = json.load(f)\n"],"metadata":{"id":"swu39S_ljkgW","executionInfo":{"status":"ok","timestamp":1754407636318,"user_tz":300,"elapsed":75,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from openai import OpenAI, OpenAIError\n","import faiss\n","import json\n","import numpy as np\n","from pathlib import Path\n","\n","# 🔐 Replace this with your actual API key (keep secret)\n","client = OpenAI(api_key=\"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\")  # or use: client = OpenAI() if key is in env\n","\n","# 📁 Paths to index + metadata\n","BASE_PATH = Path(\"/content/drive/MyDrive/Ethos LLM/Project_Root/06_LLM_Knowledge_Base/_metadata\")\n","INDEX_PATH = BASE_PATH / \"document_index.faiss\"\n","METADATA_PATH = BASE_PATH / \"document_metadata.json\"\n","\n","# 🔍 Query the knowledgebase\n","def query_knowledgebase(question, k=5):\n","    try:\n","        # 🔹 Embed the user question using OpenAI's new SDK\n","        response = client.embeddings.create(\n","            input=question,\n","            model=\"text-embedding-3-small\"\n","        )\n","        query_embedding = response.data[0].embedding\n","    except OpenAIError as e:\n","        print(f\"❌ Embedding failed: {e}\")\n","        return None\n","\n","    # 🔹 Load FAISS index and metadata\n","    index = faiss.read_index(str(INDEX_PATH))\n","    with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as f:\n","        metadata = json.load(f)\n","\n","    # 🔹 Search FAISS with the question embedding\n","    D, I = index.search(np.array([query_embedding], dtype=\"float32\"), k)\n","\n","    # 🔹 Collect top-k matching chunks\n","    results = []\n","    for i in I[0]:\n","        if i < len(metadata):\n","            entry = metadata[i]\n","            results.append({\n","                \"source\": f\"{entry['category']} → {entry['filename']}\",\n","                \"text\": entry['text']\n","            })\n","\n","    return results\n"],"metadata":{"id":"EB1PVzVPkgPM","executionInfo":{"status":"ok","timestamp":1754407636365,"user_tz":300,"elapsed":42,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def answer_with_reasoning(question, chunks):\n","    context = \"\\n\\n\".join([f\"Source:\\n{c['text']}\" for c in chunks])\n","\n","    glossary = \"\"\"\n","Glossary (for this domain):\n","\n","- E&O: Excess and Obsolete Inventory\n","- S&OP: Sales and Operations Planning\n","- ATP: Available to Promise\n","- WIP: Work in Progress\n","- MRP: Material Requirements Planning\n","- MOQ: Minimum Order Quantity\n","- SKU Rationalization: Reducing redundant SKUs to optimize efficiency\n","- Forecast Bias: Systematic error in demand forecasting\n","- Inventory Turns: Frequency of inventory turnover in a given period\n","\"\"\"\n","\n","    prompt = f\"\"\"\n","You are a supply chain expert assistant answering questions using trusted internal documents and domain-specific reasoning.\n","\n","Use the glossary below to interpret key terms. If unfamiliar acronyms or supply chain terminology appear in the context or question, you should infer their meaning as part of the supply chain or S&OP domain unless clearly stated otherwise.\n","\n","Base your answer on the following context. If the answer is not directly stated, infer using best practices in supply chain and inventory management.\n","\n","Context:\n","{context}\n","\n","Question:\n","{question}\n","\n","Your answer should:\n","- Be thoughtful and complete\n","- Base your answer on both context and best practices\n","- Use glossary definitions when available\n","- Assume any unknown terms are part of supply chain terminology unless clearly stated otherwise\n","- Clearly separate direct evidence from inferences\n","\n","Answer:\n","\"\"\"\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-4\",\n","        messages=[{\"role\": \"user\", \"content\": prompt}],\n","        temperature=0.3\n","    )\n","\n","    return response.choices[0].message.content.strip()\n"],"metadata":{"id":"R0ZOIh_vrBGt","executionInfo":{"status":"ok","timestamp":1754407636387,"user_tz":300,"elapsed":19,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["question = \"what are root causes of poor ATP accuracy?\"\n","\n","results = query_knowledgebase(question, k=15)\n","answer = answer_with_reasoning(question, results)\n","print(answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoj59kyYrGsg","executionInfo":{"status":"ok","timestamp":1754408076477,"user_tz":300,"elapsed":18069,"user":{"displayName":"Dan Skemp","userId":"10186808462358270286"}},"outputId":"301ba390-bb4b-4d9e-8837-ee2efb14dbdb"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Direct Evidence:\n","The context provides several potential causes of poor ATP (Available to Promise) accuracy. These include:\n","\n","1. Data errors: These can come from various sources such as numeric transpositions, typos, missing or incomplete data, older or not fully integrated databases with multiple versions of a record, and redundant databases in the network or different tags for the same objects. \n","\n","2. Delays in data collection: This can mean that the data arrive too late to be relevant.\n","\n","3. Misidentification of product and units of measure: During annual inventories, misidentification often occurs because inexperienced counters assisting with the effort do not recognize items, misunderstand package descriptions, and so on.\n","\n","4. Discrepancies “adjusted away”: If the reason for a discrepancy cannot be immediately found during the inventory, an adjustment is made with the underlying cause of the error never being corrected.\n","\n","Inferences:\n","Based on best practices in supply chain and inventory management, other potential root causes of poor ATP accuracy could include:\n","\n","1. Inaccurate forecasting: Poor forecast accuracy can lead to incorrect ATP calculations, as ATP is based on expected inventory availability.\n","\n","2. Inefficient supply chain operations: If there are inefficiencies or delays in production, procurement, or delivery, these can affect the accuracy of ATP calculations.\n","\n","3. Lack of real-time visibility: ATP calculations require up-to-date information about inventory levels, demand, and supply. Lack of real-time visibility into these factors can result in inaccurate ATP calculations.\n","\n","4. Poor data management: This includes not only data accuracy but also data completeness, timeliness, and consistency. Without proper data management, ATP calculations may be based on incorrect or incomplete data.\n","\n","5. Inadequate technology systems: If the systems used for ATP calculations are outdated or not robust enough, this can lead to inaccuracies. \n","\n","6. Inadequate training or skills: If the staff responsible for ATP calculations do not have the necessary training or skills, this can lead to errors.\n"]}]}]}